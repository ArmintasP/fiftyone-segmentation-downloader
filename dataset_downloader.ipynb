{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.labels as foul\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataset(dir, class_name, dataset):\n",
    "    classes = dataset.default_classes\n",
    "    mask_targets = {i: label for i, label in enumerate(classes)}\n",
    "\n",
    "    # Render instance segmentations in `Segmentation` format\n",
    "    foul.objects_to_segmentations(\n",
    "        dataset,\n",
    "        \"ground_truth\",\n",
    "        \"segmentation\",\n",
    "        mask_targets=mask_targets,\n",
    "    )\n",
    "\n",
    "    # Export images and detection XML\n",
    "    dataset.export(\n",
    "        data_path= os.path.join(dir, class_name, \"images\"),\n",
    "        #labels_path= os.path.join(dir, class_name, \"labels\")\n",
    "        dataset_type=fo.types.VOCDetectionDataset,\n",
    "        label_field=\"ground_truth\",\n",
    "    )\n",
    "\n",
    "    # Export segmentations\n",
    "    dataset.export(\n",
    "        labels_path= os.path.join(dir, class_name, \"masks\"),\n",
    "        dataset_type=fo.types.ImageSegmentationDirectory,\n",
    "        label_field=\"segmentation\",\n",
    "    )\n",
    "    \n",
    "def download_dataset(samples_count, class_name, is_train, dir, seed = 51):\n",
    "    # We use \"test\" for training, because \"test\" has bigger data and takes less to download.\n",
    "    dir = os.path.join(dir, \"train\" if is_train else \"validation\")\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v7\",\n",
    "        split= \"test\" if is_train else \"validation\",\n",
    "        label_types=[\"segmentations\"],\n",
    "        classes = [class_name],\n",
    "        max_samples= samples_count,\n",
    "        seed=seed,\n",
    "        shuffle=True,\n",
    "        dataset_name=str(uuid.uuid4()),\n",
    "        cleanup=True\n",
    "        )\n",
    "    \n",
    "    export_dataset(dir, class_name, dataset)\n",
    "    dataset.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'test' to '/home/cat/fiftyone/open-images-v7/test' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Loading existing dataset 'a'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Computing metadata...\n",
      " 100% |█████████████████| 154/154 [298.4ms elapsed, 0s remaining, 516.2 samples/s]      \n",
      " 100% |█████████████████| 154/154 [4.8s elapsed, 0s remaining, 30.3 samples/s]      \n",
      " 100% |█████████████████| 154/154 [1.2s elapsed, 0s remaining, 130.4 samples/s]         \n",
      " 100% |█████████████████| 154/154 [2.6s elapsed, 0s remaining, 60.8 samples/s]      \n",
      "Downloading split 'validation' to '/home/cat/fiftyone/open-images-v7/validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 100/100 [2.4s elapsed, 0s remaining, 40.3 samples/s]      \n",
      "Dataset 'a' created\n",
      "Computing metadata...\n",
      " 100% |█████████████████| 100/100 [214.8ms elapsed, 0s remaining, 465.6 samples/s]     \n",
      " 100% |█████████████████| 100/100 [9.2s elapsed, 0s remaining, 15.2 samples/s]      \n",
      "Directory '/home/cat/Documents/t3/validation/Cat/images' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 100/100 [1.8s elapsed, 0s remaining, 81.0 samples/s]      \n",
      "Directory '/home/cat/Documents/t3/validation/Cat/masks' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 100/100 [2.1s elapsed, 0s remaining, 65.4 samples/s]      \n",
      "Downloading split 'test' to '/home/cat/fiftyone/open-images-v7/test' if necessary\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m download_dataset(samples_count\u001b[39m=\u001b[39m\u001b[39m350\u001b[39m, class_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCat\u001b[39m\u001b[39m\"\u001b[39m, is_train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mdir_path)\n\u001b[1;32m      3\u001b[0m download_dataset(samples_count\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, class_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCat\u001b[39m\u001b[39m\"\u001b[39m, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mdir_path)\n\u001b[0;32m----> 4\u001b[0m download_dataset(samples_count\u001b[39m=\u001b[39;49m\u001b[39m350\u001b[39;49m, class_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDog\u001b[39;49m\u001b[39m\"\u001b[39;49m, is_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49mdir_path)\n\u001b[1;32m      5\u001b[0m download_dataset(samples_count\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, class_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDog\u001b[39m\u001b[39m\"\u001b[39m, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mdir_path)\n\u001b[1;32m      6\u001b[0m download_dataset(samples_count\u001b[39m=\u001b[39m\u001b[39m350\u001b[39m, class_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBird\u001b[39m\u001b[39m\"\u001b[39m, is_train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mdir_path)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mdownload_dataset\u001b[0;34m(samples_count, class_name, is_train, dir, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_dataset\u001b[39m(samples_count, class_name, is_train, \u001b[39mdir\u001b[39m, seed \u001b[39m=\u001b[39m \u001b[39m51\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# We use \"test\" for training, because \"test\" has bigger data and takes less to download.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_train \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     dataset \u001b[39m=\u001b[39m foz\u001b[39m.\u001b[39;49mload_zoo_dataset(\n\u001b[1;32m     32\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mopen-images-v7\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m         split\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m is_train \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m         label_types\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39msegmentations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     35\u001b[0m         classes \u001b[39m=\u001b[39;49m [class_name],\n\u001b[1;32m     36\u001b[0m         max_samples\u001b[39m=\u001b[39;49m samples_count,\n\u001b[1;32m     37\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     38\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m         dataset_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m         cleanup\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     export_dataset(\u001b[39mdir\u001b[39m, class_name, dataset)\n\u001b[1;32m     44\u001b[0m     dataset\u001b[39m.\u001b[39mdelete()\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:251\u001b[0m, in \u001b[0;36mload_zoo_dataset\u001b[0;34m(name, split, splits, label_field, dataset_name, dataset_dir, download_if_necessary, drop_existing_dataset, overwrite, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     zoo_dataset_cls \u001b[39m=\u001b[39m _get_zoo_dataset_cls(name)\n\u001b[1;32m    247\u001b[0m     download_kwargs, _ \u001b[39m=\u001b[39m fou\u001b[39m.\u001b[39mextract_kwargs_for_class(\n\u001b[1;32m    248\u001b[0m         zoo_dataset_cls, kwargs\n\u001b[1;32m    249\u001b[0m     )\n\u001b[0;32m--> 251\u001b[0m     info, dataset_dir \u001b[39m=\u001b[39m download_zoo_dataset(\n\u001b[1;32m    252\u001b[0m         name,\n\u001b[1;32m    253\u001b[0m         splits\u001b[39m=\u001b[39;49msplits,\n\u001b[1;32m    254\u001b[0m         dataset_dir\u001b[39m=\u001b[39;49mdataset_dir,\n\u001b[1;32m    255\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    256\u001b[0m         cleanup\u001b[39m=\u001b[39;49mcleanup,\n\u001b[1;32m    257\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_kwargs,\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     zoo_dataset \u001b[39m=\u001b[39m info\u001b[39m.\u001b[39mget_zoo_dataset()\n\u001b[1;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:170\u001b[0m, in \u001b[0;36mdownload_zoo_dataset\u001b[0;34m(name, split, splits, dataset_dir, overwrite, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads the dataset of the given name from the FiftyOne Dataset Zoo.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[39mAny dataset splits that already exist in the specified directory are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m    -   dataset_dir: the directory containing the dataset\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m zoo_dataset, dataset_dir \u001b[39m=\u001b[39m _parse_dataset_details(\n\u001b[1;32m    168\u001b[0m     name, dataset_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    169\u001b[0m )\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m zoo_dataset\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m    171\u001b[0m     dataset_dir\u001b[39m=\u001b[39;49mdataset_dir,\n\u001b[1;32m    172\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m    173\u001b[0m     splits\u001b[39m=\u001b[39;49msplits,\n\u001b[1;32m    174\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    175\u001b[0m     cleanup\u001b[39m=\u001b[39;49mcleanup,\n\u001b[1;32m    176\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:1076\u001b[0m, in \u001b[0;36mZooDataset.download_and_prepare\u001b[0;34m(self, dataset_dir, split, splits, overwrite, cleanup)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1066\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDownloading split \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1067\u001b[0m         split,\n\u001b[1;32m   1068\u001b[0m         split_dir,\n\u001b[1;32m   1069\u001b[0m         suffix,\n\u001b[1;32m   1070\u001b[0m     )\n\u001b[1;32m   1072\u001b[0m (\n\u001b[1;32m   1073\u001b[0m     dataset_type,\n\u001b[1;32m   1074\u001b[0m     num_samples,\n\u001b[1;32m   1075\u001b[0m     classes,\n\u001b[0;32m-> 1076\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(split_dir, scratch_dir, split)\n\u001b[1;32m   1078\u001b[0m \u001b[39m# Add split to ZooDatasetInfo\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/zoo/datasets/base.py:2685\u001b[0m, in \u001b[0;36mOpenImagesV7Dataset._download_and_prepare\u001b[0;34m(self, dataset_dir, _, split)\u001b[0m\n\u001b[1;32m   2684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dataset_dir, _, split):\n\u001b[0;32m-> 2685\u001b[0m     num_samples, classes, downloaded \u001b[39m=\u001b[39m fouo\u001b[39m.\u001b[39;49mdownload_open_images_split(\n\u001b[1;32m   2686\u001b[0m         dataset_dir,\n\u001b[1;32m   2687\u001b[0m         split,\n\u001b[1;32m   2688\u001b[0m         label_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_types,\n\u001b[1;32m   2689\u001b[0m         classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses,\n\u001b[1;32m   2690\u001b[0m         attrs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs,\n\u001b[1;32m   2691\u001b[0m         image_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_ids,\n\u001b[1;32m   2692\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[1;32m   2693\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle,\n\u001b[1;32m   2694\u001b[0m         seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m   2695\u001b[0m         max_samples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples,\n\u001b[1;32m   2696\u001b[0m         version\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv7\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2697\u001b[0m     )\n\u001b[1;32m   2699\u001b[0m     dataset_type \u001b[39m=\u001b[39m fot\u001b[39m.\u001b[39mOpenImagesV7Dataset()\n\u001b[1;32m   2701\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded:\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/utils/openimages.py:759\u001b[0m, in \u001b[0;36mdownload_open_images_split\u001b[0;34m(dataset_dir, split, version, label_types, classes, attrs, image_ids, num_workers, shuffle, seed, max_samples)\u001b[0m\n\u001b[1;32m    757\u001b[0m     did_download \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m _did_download\n\u001b[1;32m    758\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 759\u001b[0m     image_ids, _did_download \u001b[39m=\u001b[39m _load_all_image_ids(\n\u001b[1;32m    760\u001b[0m         dataset_dir, split\u001b[39m=\u001b[39;49msplit, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    762\u001b[0m     did_download \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m _did_download\n\u001b[1;32m    764\u001b[0m downloaded_ids \u001b[39m=\u001b[39m _get_downloaded_image_ids(dataset_dir)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/utils/openimages.py:1869\u001b[0m, in \u001b[0;36m_load_all_image_ids\u001b[0;34m(dataset_dir, split, download)\u001b[0m\n\u001b[1;32m   1864\u001b[0m quiet \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1865\u001b[0m did_download \u001b[39m=\u001b[39m _download_file_if_necessary(\n\u001b[1;32m   1866\u001b[0m     csv_filepath, url, quiet\u001b[39m=\u001b[39mquiet, download\u001b[39m=\u001b[39mdownload\n\u001b[1;32m   1867\u001b[0m )\n\u001b[0;32m-> 1869\u001b[0m csv_data \u001b[39m=\u001b[39m _parse_csv(csv_filepath)\n\u001b[1;32m   1870\u001b[0m image_ids \u001b[39m=\u001b[39m [i[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m csv_data[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m   1872\u001b[0m \u001b[39mreturn\u001b[39;00m image_ids, did_download\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/utils/openimages.py:1072\u001b[0m, in \u001b[0;36m_parse_csv\u001b[0;34m(filename, dataframe, index_col)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m             reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(csvfile)\n\u001b[0;32m-> 1072\u001b[0m         data \u001b[39m=\u001b[39m [row \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader]\n\u001b[1;32m   1074\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.9/site-packages/fiftyone/utils/openimages.py:1072\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m             reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(csvfile)\n\u001b[0;32m-> 1072\u001b[0m         data \u001b[39m=\u001b[39m [row \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader]\n\u001b[1;32m   1074\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_path = \"/home/cat/Documents/t3\"\n",
    "download_dataset(samples_count=350, class_name=\"Cat\", is_train=True, dir=dir_path)\n",
    "download_dataset(samples_count=100, class_name=\"Cat\", is_train=False, dir=dir_path)\n",
    "download_dataset(samples_count=350, class_name=\"Dog\", is_train=True, dir=dir_path)\n",
    "download_dataset(samples_count=100, class_name=\"Dog\", is_train=False, dir=dir_path)\n",
    "download_dataset(samples_count=350, class_name=\"Bird\", is_train=True, dir=dir_path)\n",
    "download_dataset(samples_count=100, class_name=\"Bird\", is_train=False, dir=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = fo.launch_app(dataset_1.view())\n",
    "# print(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
